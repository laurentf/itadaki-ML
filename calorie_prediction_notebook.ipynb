{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prédiction du niveau calorique des recettes avec Random Forest\n",
        "\n",
        "Ce notebook utilise un classifieur Random Forest pour prédire le niveau calorique des recettes (BAS/MOYEN/HAUT) basé sur les ingrédients et instructions, avec préprocessing NLP et interprétation SHAP.\n",
        "\n",
        "## Objectifs:\n",
        "- Classifier les recettes en 3 niveaux caloriques (bas < 250, moyen 250-500, haut > 500)\n",
        "- Utiliser Random Forest avec bonnes pratiques\n",
        "- Préprocessing NLP des ingrédients et instructions\n",
        "- Interprétation avec SHAP (explicabilité très importante dans la nutrition)\n",
        "\n",
        "## Bloc 3:\n",
        "- C1 : Identifier le modèle de classification le plus adapté à une variable cible qualitative à 3 classes.\n",
        "- C2 : Utiliser des techniques de vectorisation pour exploiter les données textuelles dans des modèles classiques.\n",
        "- C3 : Mettre en place un pipeline de classification robuste, avec évaluation et sauvegarde."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.stats import randint\n",
        "import shap\n",
        "import re\n",
        "import ast\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuration thème sombre harmonisé\n",
        "plt.style.use('dark_background')\n",
        "plt.rcParams['axes.unicode_minus'] = False\n",
        "\n",
        "# Palette de couleurs magnifique\n",
        "beautiful_colors = ['#FF6B9D', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', '#FF9FF3', '#54A0FF', '#5F27CD', '#A8E6CF', '#FFD93D']\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"Libraries importées avec succès avec thème sombre harmonisé!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Chargement et exploration des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chargement des données\n",
        "df = pd.read_csv('data/RAW_recipes.csv')\n",
        "print(f\"Forme du dataset: {df.shape}\")\n",
        "print(f\"\\nColonnes: {df.columns.tolist()}\")\n",
        "print(f\"\\nPremières lignes:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Informations sur le dataset\n",
        "df.info()\n",
        "print(\"\\nValeurs manquantes:\")\n",
        "print(df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Préprocessing des données nutritionnelles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_nutrition(nutrition_str):\n",
        "    \"\"\"Parse la colonne nutrition pour extraire les valeurs nutritionnelles\"\"\"\n",
        "    try:\n",
        "        # Convertir la chaîne en liste\n",
        "        nutrition_list = ast.literal_eval(nutrition_str)\n",
        "        return nutrition_list\n",
        "    except:\n",
        "        return [0, 0, 0, 0, 0, 0, 0]\n",
        "\n",
        "# Appliquer le parsing\n",
        "df['nutrition_parsed'] = df['nutrition'].apply(parse_nutrition)\n",
        "\n",
        "# Extraire les valeurs nutritionnelles (l'ordre est: calories, total_fat, sugar, sodium, protein, saturated_fat, carbohydrates)\n",
        "nutrition_columns = ['calories', 'total_fat', 'sugar', 'sodium', 'protein', 'saturated_fat', 'carbohydrates']\n",
        "for i, col in enumerate(nutrition_columns):\n",
        "    df[col] = df['nutrition_parsed'].apply(lambda x: x[i] if len(x) > i else 0)\n",
        "\n",
        "# Supprimer les valeurs aberrantes de calories (> 3000 ou < 0)\n",
        "df = df[(df['calories'] >= 0) & (df['calories'] <= 3000)]\n",
        "\n",
        "print(f\"Statistiques des calories après nettoyage:\")\n",
        "print(df['calories'].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Analyse descriptive des calories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_calories(cal):\n",
        "    \"\"\"Classifie les calories en trois catégories\"\"\"\n",
        "    if cal < 250:\n",
        "        return 'bas'\n",
        "    elif cal <= 500:\n",
        "        return 'moyen'\n",
        "    else:\n",
        "        return 'haut'\n",
        "\n",
        "# Créer la variable cible\n",
        "df['calorie_level'] = df['calories'].apply(classify_calories)\n",
        "\n",
        "# Visualisation harmonisée de la distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
        "fig.patch.set_facecolor('#1a1a1a')  # Fond général\n",
        "\n",
        "# 1. Distribution des calories avec style harmonisé\n",
        "axes[0].set_facecolor('#2d2d2d')  # Fond du subplot\n",
        "n, bins, patches = axes[0].hist(df['calories'], bins=50, alpha=0.9, \n",
        "                               edgecolor='white', linewidth=0.5)\n",
        "\n",
        "# Appliquer un dégradé de couleurs\n",
        "for i, patch in enumerate(patches):\n",
        "    # Créer un dégradé avec la couleur principale\n",
        "    base_color = np.array([255, 107, 157])  # #FF6B9D en RGB\n",
        "    intensity = 0.3 + 0.7 * (i / len(patches))\n",
        "    color = base_color * intensity / 255.0\n",
        "    patch.set_facecolor(color)\n",
        "\n",
        "axes[0].set_title('Distribution des calories dans les recettes', fontweight='bold', \n",
        "                 fontsize=16, color='white', pad=20)\n",
        "axes[0].set_xlabel('Calories', fontweight='bold', color='white', fontsize=12)\n",
        "axes[0].set_ylabel('Fréquence', fontweight='bold', color='white', fontsize=12)\n",
        "\n",
        "# Lignes de seuil avec couleurs contrastantes\n",
        "axes[0].axvline(x=250, color='#FFD93D', linestyle='--', linewidth=3, \n",
        "               alpha=0.9, label='Seuil bas (250)')\n",
        "axes[0].axvline(x=500, color='#4ECDC4', linestyle='--', linewidth=3, \n",
        "               alpha=0.9, label='Seuil moyen (500)')\n",
        "\n",
        "axes[0].tick_params(colors='white')\n",
        "axes[0].grid(True, alpha=0.3, color='#404040', linestyle='--')\n",
        "\n",
        "# Légende harmonisée\n",
        "legend = axes[0].legend(framealpha=0.9, facecolor='#2d2d2d', \n",
        "                       edgecolor='white', fontsize=11)\n",
        "for text in legend.get_texts():\n",
        "    text.set_color('white')\n",
        "\n",
        "# Style des bordures\n",
        "for spine in axes[0].spines.values():\n",
        "    spine.set_color('#404040')\n",
        "\n",
        "# 2. Distribution des niveaux caloriques avec palette harmonisée\n",
        "axes[1].set_facecolor('#2d2d2d')\n",
        "\n",
        "calorie_counts = df['calorie_level'].value_counts()\n",
        "# Ordre logique des catégories\n",
        "ordered_levels = ['bas', 'moyen', 'haut']\n",
        "ordered_counts = [calorie_counts.get(level, 0) for level in ordered_levels]\n",
        "\n",
        "# Couleurs spécifiques pour chaque catégorie\n",
        "level_colors = ['#96CEB4', '#4ECDC4', '#FF6B9D']  # Vert, Cyan, Rose\n",
        "\n",
        "bars = axes[1].bar(ordered_levels, ordered_counts, \n",
        "                   color=level_colors, alpha=0.9, \n",
        "                   edgecolor='white', linewidth=1.5)\n",
        "\n",
        "axes[1].set_title('Distribution des classes (niveaux caloriques)', \n",
        "                 fontweight='bold', fontsize=16, color='white', pad=20)\n",
        "axes[1].set_xlabel('Niveau Calorique', fontweight='bold', color='white', fontsize=12)\n",
        "axes[1].set_ylabel('Nombre de Recettes', fontweight='bold', color='white', fontsize=12)\n",
        "\n",
        "# Ajouter les valeurs sur les barres avec style\n",
        "for i, (bar, count) in enumerate(zip(bars, ordered_counts)):\n",
        "    percentage = (count / sum(ordered_counts)) * 100\n",
        "    axes[1].text(bar.get_x() + bar.get_width()/2, \n",
        "                bar.get_height() + max(ordered_counts)*0.02,\n",
        "                f'{count:,}\\n({percentage:.1f}%)', \n",
        "                ha='center', va='bottom', fontweight='bold', \n",
        "                color='white', fontsize=11)\n",
        "\n",
        "axes[1].tick_params(colors='white')\n",
        "axes[1].grid(True, alpha=0.3, color='#404040', linestyle='--')\n",
        "\n",
        "# Style des bordures\n",
        "for spine in axes[1].spines.values():\n",
        "    spine.set_color('#404040')\n",
        "\n",
        "# Ajustement harmonisé avec titre général\n",
        "fig.suptitle('ANALYSE DESCRIPTIVE DES CALORIES', \n",
        "             fontsize=20, fontweight='bold', color='white', y=0.98)\n",
        "\n",
        "plt.tight_layout(pad=3.0, rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "print(\"Distribution des niveaux caloriques:\")\n",
        "print(df['calorie_level'].value_counts())\n",
        "print(\"\\nPourcentages:\")\n",
        "print(df['calorie_level'].value_counts(normalize=True) * 100)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Préprocessing NLP des ingrédients et instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_text_simple(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convertir en string si ce n'est pas déjà fait\n",
        "    text = str(text)\n",
        "    \n",
        "    # Supprimer les crochets et guillemets\n",
        "    text = re.sub(r\"[\\[\\]'\\\"]\", \"\", text)\n",
        "    \n",
        "    # Remplacer les virgules par des séparateurs pipe\n",
        "    text = re.sub(r\",\", \" | \", text)\n",
        "    \n",
        "    # Supprimer les caractères spéciaux sauf les espaces et pipes\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s|]\", \"\", text)\n",
        "    \n",
        "    # Convertir en minuscules\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Supprimer les espaces multiples\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    \n",
        "    # Nettoyer les espaces autour des pipes\n",
        "    text = re.sub(r\"\\s*\\|\\s*\", \" | \", text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Téléchargement des ressources NLTK\n",
        "print(\"Téléchargement des ressources NLTK...\")\n",
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)  # Version récente de punkt\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)  # Pour WordNet multilingue\n",
        "print(\"Ressources NLTK téléchargées!\")\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Configuration NLTK avec stop words culinaires personnalisés\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "print(f\"Stop words enrichis: {len(stop_words)} mots au total\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Nettoyage avancé du texte avec NLTK, lemmatisation et tri alphabétique\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or text == '':\n",
        "        return ''\n",
        "    \n",
        "    # Convertir en string et minuscules\n",
        "    text = str(text).lower()\n",
        "    \n",
        "    # Supprimer les crochets et guillemets d'abord\n",
        "    text = re.sub(r\"[\\[\\]'\\\"]\", \"\", text)\n",
        "    \n",
        "    # Remplacer les virgules par des séparateurs pipe\n",
        "    text = re.sub(r\",\", \" | \", text)\n",
        "\n",
        "    # Supprimer la ponctuation et caractères spéciaux\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    \n",
        "    # Supprimer les chiffres (quantités)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    \n",
        "    # Tokenisation\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Supprimer stopwords et mots courts (< 3 caractères)\n",
        "    tokens = [token for token in tokens if token not in stop_words and (len(token) > 2 or token.isdigit())]\n",
        "    \n",
        "    # Lemmatisation\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    \n",
        "    # Supprimer les doublons et trier alphabétiquement\n",
        "    # unique_tokens = sorted(set(tokens))\n",
        "    unique_tokens = set(tokens)\n",
        "    \n",
        "    return ' '.join(unique_tokens)\n",
        "\n",
        "def sort_ingredients(ingredients_text):\n",
        "    \"\"\"\n",
        "    Trie les ingrédients par ordre alphabétique avant nettoyage\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convertir la chaîne en liste\n",
        "        ingredients_list = ast.literal_eval(ingredients_text)\n",
        "        \n",
        "        # Trier par ordre alphabétique\n",
        "        sorted_ingredients = sorted(ingredients_list)\n",
        "        \n",
        "        # Retourner comme chaîne\n",
        "        return str(sorted_ingredients)\n",
        "    except:\n",
        "        # Si échec, retourner tel quel\n",
        "        return ingredients_text\n",
        "\n",
        "# Test de la fonction optimisée\n",
        "test_ingredient = \"['2 cups chopped fresh spinach', '1 large diced tomato', '3 cloves minced garlic', '1/4 cup olive oil']\"\n",
        "print(f\"\\nTest de la fonction optimisée:\")\n",
        "print(f\"Avant: {test_ingredient}\")\n",
        "print(f\"Après: {clean_text(test_ingredient)}\")\n",
        "\n",
        "# Nettoyer les ingrédients avec la fonction optimisée\n",
        "print(f\"\\nNettoyage des ingrédients en cours...\")\n",
        "# df['ingredients_sorted'] = df['steps'].apply(sort_ingredients)\n",
        "# df['ingredients_cleaned'] = df['ingredients_sorted'].apply(clean_text_simple)\n",
        "\n",
        "df['steps_sorted'] = df['steps'].apply(sort_ingredients)\n",
        "df['ingredients_cleaned'] = df['steps_sorted'].apply(clean_text)\n",
        "\n",
        "# Supprimer les recettes avec du texte vide\n",
        "df = df[df['ingredients_cleaned'].str.len() > 10]\n",
        "\n",
        "print(f\"Nombre de recettes après nettoyage avancé: {len(df)}\")\n",
        "print(\"\\nExemple de texte nettoyé et optimisé:\")\n",
        "print(df['ingredients_cleaned'].iloc[0][:200] + \"...\")\n",
        "\n",
        "# Statistiques d'amélioration\n",
        "print(f\"\\nStatistiques d'amélioration:\")\n",
        "word_counts = df['ingredients_cleaned'].apply(lambda x: len(x.split()))\n",
        "print(f\"Nombre moyen de mots par recette: {word_counts.mean():.1f}\")\n",
        "print(f\"Nombre médian de mots par recette: {word_counts.median():.1f}\")\n",
        "print(f\"Recettes avec moins de 5 mots: {(word_counts < 5).sum():,}\")\n",
        "print(f\"Recettes avec 5-15 mots: {((word_counts >= 5) & (word_counts <= 15)).sum():,}\")\n",
        "print(f\"Recettes avec plus de 15 mots: {(word_counts > 15).sum():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Vectorisation TF-IDF et préparation des données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LabelEncoder pour les classes, et en plus ça garantit l'ordre des classes\n",
        "le = LabelEncoder()\n",
        "le.fit(['bas', 'moyen', 'haut'])\n",
        "\n",
        "y_encoded = le.transform(df['calorie_level'])\n",
        "\n",
        "# Paramètres TF-IDF optimisés\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=10000,  # Limiter le nombre de features\n",
        "    min_df=200,          # Ignorer les termes qui apparaissent dans moins de 0.1% documents\n",
        "    max_df=0.7,        # Ignorer les termes qui apparaissent dans plus de 80% des documents\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2)  # Utiliser des unigrammes et bigrammes\n",
        ")\n",
        "\n",
        "# Vectoriser le texte\n",
        "X_tfidf = tfidf.fit_transform(df['ingredients_cleaned'])\n",
        "print(f\"Forme de la matrice TF-IDF: {X_tfidf.shape}\")\n",
        "\n",
        "# Préparer les variables\n",
        "X = X_tfidf\n",
        "y = y_encoded\n",
        "\n",
        "# Division des données\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Taille du jeu d'entraînement: {X_train.shape[0]}\")\n",
        "print(f\"Taille du jeu de test: {X_test.shape[0]}\")\n",
        "print(f\"\\nDistribution dans le jeu d'entraînement:\")\n",
        "for i, classe in enumerate(le.classes_):\n",
        "    count = (y_train == i).sum()\n",
        "    percentage = (y_train == i).mean() * 100\n",
        "    print(f\"{classe}: {count:,} ({percentage:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Optimisation des Hyperparamètres avec SearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Initialisation du SearchCV...\")\n",
        "\n",
        "param = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [15, 20, 25],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'class_weight': ['balanced'],\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Créer le modèle de base\n",
        "rf_base = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Recherche par grille avec validation croisée réduite\n",
        "print(\"Optimisation des hyperparamètres en cours...\")\n",
        "print(f\"Nombre de combinaisons: {len(param['n_estimators']) * len(param['max_depth']) * len(param['min_samples_split']) * len(param['min_samples_leaf']) * len(param['max_features'])}\")\n",
        "\n",
        "# RandomizedSearchCV pour 20 itérations\n",
        "search = RandomizedSearchCV(\n",
        "    rf_base,\n",
        "    param_distributions=param,\n",
        "    n_iter=20,\n",
        "    cv=3,\n",
        "    scoring='balanced_accuracy',\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nMeilleurs paramètres: {search.best_params_}\")\n",
        "print(f\"Meilleur score de validation croisée: {search.best_score_:.4f}\")\n",
        "\n",
        "# Utiliser le meilleur modèle\n",
        "best_rf = search.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Évaluation du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prédictions\n",
        "y_pred_train = best_rf.predict(X_train)\n",
        "y_pred_test = best_rf.predict(X_test)\n",
        "\n",
        "# Scores d'accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
        "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Accuracy d'entraînement: {train_accuracy:.4f}\")\n",
        "print(f\"Accuracy de test: {test_accuracy:.4f}\")\n",
        "\n",
        "# Rapport de classification\n",
        "print(\"\\nRapport de classification (jeu de test):\")\n",
        "print(classification_report(y_test, y_pred_test, target_names=['bas', 'moyen', 'haut']))\n",
        "\n",
        "# Matrice de confusion harmonisée\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "fig.patch.set_facecolor('#1a1a1a')\n",
        "ax.set_facecolor('#2d2d2d')\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "# Créer un heatmap personnalisé avec des couleurs harmonisées\n",
        "# Utiliser une palette dégradée du thème\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# Créer une colormap personnalisée\n",
        "colors = ['#2d2d2d', '#45B7D1', '#4ECDC4', '#FF6B9D']\n",
        "n_bins = 100\n",
        "cmap = mcolors.LinearSegmentedColormap.from_list('custom', colors, N=n_bins)\n",
        "\n",
        "# Heatmap avec style personnalisé\n",
        "im = ax.imshow(cm, interpolation='nearest', cmap=cmap, alpha=0.9)\n",
        "\n",
        "# Annotations avec style\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        text_color = 'white' if cm[i, j] > cm.max() / 2 else 'black'\n",
        "        ax.text(j, i, f'{cm[i, j]}\\n({cm[i, j]/cm.sum()*100:.1f}%)',\n",
        "                ha='center', va='center', fontweight='bold',\n",
        "                color=text_color, fontsize=14)\n",
        "\n",
        "# Labels harmonisés\n",
        "class_names = ['bas', 'moyen', 'haut']\n",
        "ax.set_xticks(range(len(class_names)))\n",
        "ax.set_yticks(range(len(class_names)))\n",
        "ax.set_xticklabels(class_names, fontsize=12, color='white', fontweight='bold')\n",
        "ax.set_yticklabels(class_names, fontsize=12, color='white', fontweight='bold')\n",
        "\n",
        "# Titres et labels\n",
        "ax.set_title('Matrice de Confusion', fontsize=18, fontweight='bold', \n",
        "             color='white', pad=20)\n",
        "ax.set_xlabel('Prédictions', fontsize=14, fontweight='bold', color='white')\n",
        "ax.set_ylabel('Valeurs Réelles', fontsize=14, fontweight='bold', color='white')\n",
        "\n",
        "# Colorbar harmonisée\n",
        "cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
        "cbar.ax.yaxis.set_tick_params(color='white')\n",
        "cbar.ax.tick_params(labelcolor='white')\n",
        "cbar.set_label('Nombre de Prédictions', color='white', fontweight='bold')\n",
        "\n",
        "# Grille subtile\n",
        "ax.set_xticks(np.arange(len(class_names) + 1) - 0.5, minor=True)\n",
        "ax.set_yticks(np.arange(len(class_names) + 1) - 0.5, minor=True)\n",
        "ax.grid(which='minor', color='white', linestyle='-', linewidth=0.5, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Validation croisée\n",
        "cv_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"\\nScores de validation croisée: {cv_scores}\")\n",
        "print(f\"Score moyen: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Importance des features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtenir les noms des features\n",
        "feature_names = tfidf.get_feature_names_out()\n",
        "feature_importance = best_rf.feature_importances_\n",
        "\n",
        "# Créer un DataFrame pour les importances\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': feature_importance\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "# Graphique harmonisé des features importantes\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "fig.patch.set_facecolor('#1a1a1a')\n",
        "ax.set_facecolor('#2d2d2d')\n",
        "\n",
        "top_features = importance_df.head(20)\n",
        "\n",
        "# Créer les barres horizontales avec palette harmonisée\n",
        "colors = [beautiful_colors[i % len(beautiful_colors)] for i in range(len(top_features))]\n",
        "bars = ax.barh(range(len(top_features)), top_features['importance'],\n",
        "               color=colors, alpha=0.9, \n",
        "               edgecolor='white', linewidth=0.8)\n",
        "\n",
        "# Configuration des axes\n",
        "ax.set_yticks(range(len(top_features)))\n",
        "ax.set_yticklabels(top_features['feature'], fontsize=11, color='white', fontweight='bold')\n",
        "ax.set_xlabel('Importance', fontweight='bold', color='white', fontsize=14)\n",
        "ax.set_title('Top 20 Features les Plus Importantes', \n",
        "             fontweight='bold', fontsize=18, color='white', pad=20)\n",
        "\n",
        "# Ajouter les valeurs sur les barres\n",
        "for i, (bar, importance) in enumerate(zip(bars, top_features['importance'])):\n",
        "    ax.text(bar.get_width() + max(top_features['importance'])*0.01, \n",
        "            bar.get_y() + bar.get_height()/2, \n",
        "            f'{importance:.4f}', \n",
        "            ha='left', va='center', fontweight='bold', \n",
        "            color='white', fontsize=10)\n",
        "\n",
        "# Style harmonisé\n",
        "ax.invert_yaxis()\n",
        "ax.tick_params(colors='white')\n",
        "ax.grid(True, alpha=0.3, color='#404040', linestyle='--', axis='x')\n",
        "\n",
        "# Bordures harmonisées\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_color('#404040')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Top 10 des features les plus importantes:\")\n",
        "print(importance_df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Analyse SHAP pour l'explicabilité"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialiser l'explainer SHAP\n",
        "print(\"Initialisation de l'explainer SHAP...\")\n",
        "explainer = shap.TreeExplainer(best_rf)\n",
        "\n",
        "# Calculer les valeurs SHAP sur un échantillon\n",
        "sample_size = min(100, X_test.shape[0])\n",
        "X_test_sample = X_test[:sample_size].toarray().astype(np.float64)  # Conversion directe\n",
        "# y_test_sample = y_test.iloc[:sample_size]\n",
        "y_test_sample = y_test[:sample_size]\n",
        "\n",
        "print(f\"Calcul des valeurs SHAP pour {sample_size} échantillons...\")\n",
        "shap_values = explainer.shap_values(X_test_sample)\n",
        "\n",
        "print(\"Analyse SHAP terminée!\")\n",
        "\n",
        "# Bar plot SHAP\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "fig.patch.set_facecolor('#1a1a1a')\n",
        "\n",
        "shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, \n",
        "                  plot_type=\"bar\", class_names=['bas', 'moyen', 'haut'], show=False)\n",
        "\n",
        "# Styles\n",
        "ax = plt.gca()\n",
        "ax.set_facecolor('#2d2d2d')\n",
        "ax.set_title('SHAP Bar Plot - Importance Moyenne des Features', \n",
        "             fontweight='bold', fontsize=18, color='white', pad=20)\n",
        "\n",
        "ax.tick_params(colors='white')\n",
        "ax.xaxis.label.set_color('white')\n",
        "ax.yaxis.label.set_color('white')\n",
        "\n",
        "for spine in ax.spines.values():\n",
        "    spine.set_color('#404040')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Prédictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_calorie_level(ingredients_text):\n",
        "    \"\"\"\n",
        "    Prédit le niveau calorique d'une recette basé sur les ingrédients\n",
        "    \n",
        "    Args:\n",
        "        ingredients_text (str): Liste des ingrédients\n",
        "    \n",
        "    Returns:\n",
        "        tuple: (prédiction, probabilités)\n",
        "    \"\"\"\n",
        "    # Nettoyer le texte\n",
        "    ingredients_cleaned = clean_text(ingredients_text)\n",
        " \n",
        "    # Vectoriser\n",
        "    text_vectorized = tfidf.transform([ingredients_cleaned])\n",
        "    \n",
        "    # Prédire\n",
        "    prediction_encoded = best_rf.predict(text_vectorized)[0]\n",
        "    probabilities = best_rf.predict_proba(text_vectorized)[0]\n",
        "    \n",
        "    # Décoder la prédiction\n",
        "    prediction = le.inverse_transform([prediction_encoded])[0]\n",
        "    \n",
        "    # Créer le dictionnaire avec les vrais noms\n",
        "    prob_dict = dict(zip(['bas', 'moyen', 'haut'], probabilities))\n",
        "    \n",
        "    return prediction, prob_dict\n",
        "\n",
        "def visualize_prediction(ingredients_text):\n",
        "    \"\"\"\n",
        "    Visualise une prédiction avec le thème harmonisé\n",
        "    \n",
        "    Args:\n",
        "        ingredients_text (str): Liste des ingrédients\n",
        "    \"\"\"\n",
        "    # Prédiction\n",
        "    prediction, prob_dict = predict_calorie_level(ingredients_text)\n",
        "    \n",
        "    # Créer la figure harmonisée\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(18, 12))\n",
        "    fig.patch.set_facecolor('#1a1a1a')\n",
        "    \n",
        "    # 1. Graphique des probabilités (camembert)\n",
        "    ax1.set_facecolor('#2d2d2d')\n",
        "    \n",
        "    # Couleurs spécifiques pour chaque catégorie\n",
        "    category_colors = {'bas': '#96CEB4', 'moyen': '#4ECDC4', 'haut': '#FF6B9D'}\n",
        "    colors = [category_colors[cat] for cat in prob_dict.keys()]\n",
        "    \n",
        "    # Mettre en évidence la prédiction\n",
        "    explode = [0.1 if cat == prediction else 0 for cat in prob_dict.keys()]\n",
        "    \n",
        "    wedges, texts, autotexts = ax1.pie(prob_dict.values(), \n",
        "                                      labels=[f'{cat.upper()}\\n{prob:.1%}' for cat, prob in prob_dict.items()],\n",
        "                                      colors=colors, explode=explode, autopct='',\n",
        "                                      shadow=True, startangle=90,\n",
        "                                      textprops={'fontsize': 12, 'color': 'white', 'fontweight': 'bold'})\n",
        "    \n",
        "    ax1.set_title(f'Prédiction: {prediction.upper()}', \n",
        "                 fontweight='bold', fontsize=16, color='white', pad=20)\n",
        "    \n",
        "    # 2. Graphique en barres des probabilités\n",
        "    ax2.set_facecolor('#2d2d2d')\n",
        "    \n",
        "    categories = list(prob_dict.keys())\n",
        "    probabilities = list(prob_dict.values())\n",
        "    colors_bars = [category_colors[cat] for cat in categories]\n",
        "    \n",
        "    bars = ax2.bar(categories, probabilities, color=colors_bars, alpha=0.9,\n",
        "                   edgecolor='white', linewidth=1.5)\n",
        "    \n",
        "    # Mettre en évidence la prédiction\n",
        "    for i, (bar, cat) in enumerate(zip(bars, categories)):\n",
        "        if cat == prediction:\n",
        "            bar.set_edgecolor('#FFD93D')\n",
        "            bar.set_linewidth(3)\n",
        "        \n",
        "        # Ajouter les valeurs\n",
        "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                f'{probabilities[i]:.1%}', ha='center', va='bottom',\n",
        "                fontweight='bold', color='white', fontsize=12)\n",
        "    \n",
        "    ax2.set_title('Probabilités par Catégorie', \n",
        "                 fontweight='bold', fontsize=16, color='white', pad=20)\n",
        "    ax2.set_ylabel('Probabilité', fontweight='bold', color='white')\n",
        "    ax2.tick_params(colors='white')\n",
        "    ax2.grid(True, alpha=0.3, color='#404040', linestyle='--')\n",
        "    \n",
        "    for spine in ax2.spines.values():\n",
        "        spine.set_color('#404040')\n",
        "    \n",
        "    # Texte des ingrédients\n",
        "    ax3.set_facecolor('#2d2d2d')\n",
        "    ax3.axis('off')\n",
        "    \n",
        "    # Nettoyer et formater les ingrédients\n",
        "    ingredients_clean = clean_text(ingredients_text)\n",
        "    ingredients_words = ingredients_clean.split()\n",
        "    \n",
        "    # Créer un texte formaté\n",
        "    ingredients_display = ', '.join(ingredients_words[:10])  # Limiter à 10 mots\n",
        "    if len(ingredients_words) > 10:\n",
        "        ingredients_display += f\"... (+{len(ingredients_words) - 10} mots)\"\n",
        "    \n",
        "    info_text = f\"\"\"ANALYSE DE LA RECETTE\n",
        "\n",
        "Prédiction: {prediction.upper()}\n",
        "Confiance: {max(prob_dict.values()):.1%}\n",
        "\n",
        "Ingrédients analysés:\n",
        "{ingredients_display}\n",
        "\n",
        "Nombre de termes: {len(ingredients_words)}\n",
        "Longueur du texte: {len(ingredients_text)} caractères\"\"\"\n",
        "    \n",
        "    ax3.text(0.05, 0.95, info_text, transform=ax3.transAxes, \n",
        "             fontsize=11, color='white', va='top', ha='left',\n",
        "             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='#404040', alpha=0.8))\n",
        "    \n",
        "    # Graphique des top features de cette prédiction\n",
        "    ax4.set_facecolor('#2d2d2d')\n",
        "    \n",
        "    # Obtenir les features de ce texte\n",
        "    text_vectorized = tfidf.transform([ingredients_clean])\n",
        "    \n",
        "    # Trouver les top features non-nulles\n",
        "    if text_vectorized.nnz > 0:  # Si il y a des features non-nulles\n",
        "        feature_indices = text_vectorized.nonzero()[1]\n",
        "        feature_scores = text_vectorized.data\n",
        "        \n",
        "        # Créer un DataFrame des features de cette prédiction\n",
        "        prediction_features = pd.DataFrame({\n",
        "            'feature': [feature_names[i] for i in feature_indices],\n",
        "            'score': feature_scores\n",
        "        }).sort_values('score', ascending=False).head(10)\n",
        "        \n",
        "        colors_features = [beautiful_colors[i % len(beautiful_colors)] for i in range(len(prediction_features))]\n",
        "        bars = ax4.barh(range(len(prediction_features)), prediction_features['score'],\n",
        "                       color=colors_features, alpha=0.9, \n",
        "                       edgecolor='white', linewidth=0.8)\n",
        "        \n",
        "        ax4.set_yticks(range(len(prediction_features)))\n",
        "        ax4.set_yticklabels(prediction_features['feature'], fontsize=10, color='white')\n",
        "        ax4.set_xlabel('Score TF-IDF', fontweight='bold', color='white')\n",
        "        ax4.set_title('Top Features de cette Recette', \n",
        "                     fontweight='bold', fontsize=14, color='white', pad=15)\n",
        "        \n",
        "        # Ajouter les valeurs\n",
        "        for i, (bar, score) in enumerate(zip(bars, prediction_features['score'])):\n",
        "            ax4.text(bar.get_width() + max(prediction_features['score'])*0.02, \n",
        "                    bar.get_y() + bar.get_height()/2, \n",
        "                    f'{score:.3f}', \n",
        "                    ha='left', va='center', fontweight='bold', \n",
        "                    color='white', fontsize=9)\n",
        "        \n",
        "        ax4.invert_yaxis()\n",
        "        ax4.tick_params(colors='white')\n",
        "        ax4.grid(True, alpha=0.3, color='#404040', linestyle='--', axis='x')\n",
        "    else:\n",
        "        ax4.text(0.5, 0.5, 'Aucune feature trouvée', ha='center', va='center',\n",
        "                transform=ax4.transAxes, color='white', fontsize=14)\n",
        "    \n",
        "    # Bordures\n",
        "    for spine in ax4.spines.values():\n",
        "        spine.set_color('#404040')\n",
        "    \n",
        "    # Titre général\n",
        "    fig.suptitle(f'PRÉDICTION: {prediction.upper()} (Confiance: {max(prob_dict.values()):.1%})', \n",
        "                 fontsize=20, fontweight='bold', color='white', y=0.98)\n",
        "    \n",
        "    plt.tight_layout(pad=3.0, rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "    \n",
        "    return prediction, prob_dict\n",
        "\n",
        "# Tester les fonctions avec quelques exemples\n",
        "print(\"EXEMPLES DE PRÉDICTIONS:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Exemple 1: Recette riche en calories\n",
        "exemple1_ingredients = \"butter, heavy cream, sugar, eggs, chocolate, flour, nuts\"\n",
        "pred1, prob1 = predict_calorie_level(exemple1_ingredients)\n",
        "print(f\"\\nExemple 1 - Dessert riche:\")\n",
        "print(f\"Ingrédients: {exemple1_ingredients}\")\n",
        "print(f\"Prédiction: {pred1}\")\n",
        "print(f\"Probabilités: {prob1}\")\n",
        "\n",
        "# Exemple 2: Recette légère\n",
        "exemple2_ingredients = \"spinach, tomatoes, onion, garlic, olive oil, herbs\"\n",
        "pred2, prob2 = predict_calorie_level(exemple2_ingredients)\n",
        "print(f\"\\nExemple 2 - Plat léger:\")\n",
        "print(f\"Ingrédients: {exemple2_ingredients}\")\n",
        "print(f\"Prédiction: {pred2}\")\n",
        "print(f\"Probabilités: {prob2}\")\n",
        "\n",
        "# Exemple 3: Recette moyenne\n",
        "exemple3_ingredients = \"chicken breast, rice, vegetables, olive oil, spices\"\n",
        "pred3, prob3 = predict_calorie_level(exemple3_ingredients)\n",
        "print(f\"\\nExemple 3 - Plat équilibré:\")\n",
        "print(f\"Ingrédients: {exemple3_ingredients}\")\n",
        "print(f\"Prédiction: {pred3}\")\n",
        "print(f\"Probabilités: {prob3}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"\\nTest de la fonction de visualisation harmonisée...\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemple avec un dessert riche\n",
        "dessert_ingredients = \"butter, heavy cream, sugar, eggs, chocolate, flour, vanilla extract, cocoa powder, nuts\"\n",
        "print(f\"\\nExemple 1 - Dessert riche:\")\n",
        "print(f\"Ingrédients: {dessert_ingredients}\")\n",
        "pred1, prob1 = visualize_prediction(dessert_ingredients)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemple avec une salade légère\n",
        "salade_ingredients = \"lettuce, tomatoes, cucumber, onion, olive oil, vinegar, herbs, salt, pepper\"\n",
        "print(f\"\\nExemple 2 - Salade légère:\")\n",
        "print(f\"Ingrédients: {salade_ingredients}\")\n",
        "pred2, prob2 = visualize_prediction(salade_ingredients)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exemple avec un plat équilibré\n",
        "plat_ingredients = \"chicken breast, rice, broccoli, carrots, olive oil, garlic, onion, soy sauce, herbs\"\n",
        "print(f\"\\nExemple 3 - Plat équilibré:\")\n",
        "print(f\"Ingrédients: {plat_ingredients}\")\n",
        "pred3, prob3 = visualize_prediction(plat_ingredients)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Visualisations harmonisées terminées!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 12. Sauvegarde du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "# Sauvegarder le modèle et le vectoriseur\n",
        "joblib.dump(best_rf, 'calorie_prediction_model.pkl')\n",
        "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "print(\"Modèle et vectoriseur sauvegardés!\")\n",
        "print(\"- calorie_prediction_model.pkl\")\n",
        "print(\"- tfidf_vectorizer.pkl\")\n",
        "\n",
        "# Pour charger plus tard:\n",
        "# loaded_model = joblib.load('calorie_prediction_model.pkl')\n",
        "# loaded_tfidf = joblib.load('tfidf_vectorizer.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Résumé des résultats et conclusions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"RÉSUMÉ DU MODÈLE DE PRÉDICTION CALORIQUE HARMONISÉ\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Dataset: {df.shape[0]:,} recettes\")\n",
        "print(f\"Features: {X.shape[1]:,} features TF-IDF\")\n",
        "print(f\"Classes: {sorted(best_rf.classes_)}\")\n",
        "print(f\"\\nPerformances:\")\n",
        "print(f\"   Accuracy d'entraînement: {train_accuracy:.4f}\")\n",
        "print(f\"   Accuracy de test: {test_accuracy:.4f}\")\n",
        "print(f\"   Score de validation croisée: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "print(f\"\\nMeilleurs hyperparamètres:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"   {param}: {value}\")\n",
        "print(f\"\\nTop 5 features les plus importantes:\")\n",
        "for i, (feature, importance) in enumerate(importance_df.head(5).values):\n",
        "    print(f\"   {i+1}. {feature}: {importance:.4f}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nCONCLUSIONS PRINCIPALES:\")\n",
        "print(\"• Le modèle Random Forest peut prédire efficacement les niveaux caloriques\")\n",
        "print(\"• Les ingrédients riches (beurre, crème, sucre) sont de bons prédicteurs de calories élevées\")\n",
        "print(\"• L'analyse SHAP permet de comprendre les contributions de chaque feature\")\n",
        "print(\"• Le modèle peut être utilisé pour évaluer de nouvelles recettes\")\n",
        "print(\"• Les bonnes pratiques ML ont été appliquées (nettoyage, validation croisée, optimisation)\")\n",
        "print(\"• Interface de visualisation harmonisée avec thème sombre et couleurs magnifiques\")\n",
        "print(\"• Fonction de prédiction interactive avec analyses détaillées\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nAMÉLIORATIONS VISUELLES:\")\n",
        "print(\"• Thème sombre harmonisé sur toutes les visualisations\")\n",
        "print(\"• Palette de couleurs magnifique et cohérente\")\n",
        "print(\"• Graphiques interactifs avec informations détaillées\")\n",
        "print(\"• Prédictions visualisées avec analyses complètes\")\n",
        "print(\"• Style moderne et professionnel\")\n",
        "print(\"=\" * 60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "itadaki_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
